{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##################\n",
    "# Document Maker #\n",
    "################################################\n",
    "# Desc   : A Nice RNN for Generating Documents #\n",
    "# Author : Abe Hoffman                         #\n",
    "# Date   : Aug 21 2017                         #\n",
    "################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from collections import namedtuple\n",
    "import string, textract\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import glob, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###################\n",
    "# I. Architecture #\n",
    "###################\n",
    "# A. Load Document (from the Corpus)\n",
    "# B. Pre-process Document\n",
    "# C. Encode the Document\n",
    "# D. Define Batch Generator\n",
    "# E. Define Model Inputs\n",
    "# F. Define LSTM Cell\n",
    "# G. Define Model Output\n",
    "# H. Define Model Loss\n",
    "# I. Define Model Optimizer\n",
    "# J. The LSTM Network\n",
    "\n",
    "######################\n",
    "# II. Implementation #\n",
    "######################\n",
    "# A. Define Hyperparameters\n",
    "# B. Model Training and Checkpoints\n",
    "# C. Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I. Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A. Load the Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A1. Sanitized Patient Document\n",
    "printable = set(string.printable)\n",
    "adoc = textract.process('12345678.rtf')\n",
    "text = filter(lambda x: x in printable, adoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\nPatient: MRN:  FIN:  \\nAge: 86 years Sex: M DOB: \\nAssociated Diagnoses: None \\nAuthor: \\n \\nBasic Information \\nTime seen: Date & time 11/15/2014 02:27:00. \\nHistory source: Patient. \\nArrival mode: Private vehicle. \\nHistory limitation: None. \\nAdditional information: Chief Complaint from Nursing Triage Note : Chief Complaint ED \\n11/15/2014 2:16 PST Chief Complaint ED SHORTNESS OF BREATH , PCP is Jennifer Cook. \\n \\nHistory of Present Illness \\nThe patient presents with shortness of breath. Patient is an 86 year old male with history of CHF, presenting to the ED with shortness of breath intermittent for one week. Patient states his shortness of breath has not become worse but he was afraid to fall back to sleep. Not on home O2. His shortness of breath is worse with walking. Denies recent fever, cough, chest pain, abdominal pain, nausea, vomiting, diarrhea. Dr. Wong at UC Davis is his cardiac surgeon, and he is supposed to have aortic valve surgery. Just mvoed here from Crescent City last week, '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# B. Pre-process Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# B1. Set the Vocabulary : Unique set of characters that are found in the document\n",
    "vocab = set(text)\n",
    "# B2. Assign each unique character an integer (starting from 0) : {'<character>': 0, ... }\n",
    "vocab_to_int = {c: i for i, c in enumerate(vocab)}\n",
    "# B3. Flip assignment so integer indicates character : {<integer>: '<character>', ...}\n",
    "int_to_vocab = dict(enumerate(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# C. Encode the Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded document shape: (7786,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 2,  1, 41, 51, 71, 58, 54, 65, 71, 23], dtype=int32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# C1. For every occurence of a character in the document, assign the integer\n",
    "# (Use syntactic sugar and convert to a Numpy array)\n",
    "encoded = np.array([vocab_to_int[c] for c in text], dtype=np.int32)\n",
    "# C2. This results in our entire document encoded as character : integers, based on the defined vocabulary\n",
    "print('Encoded document shape: {}'.format(encoded.shape))\n",
    "encoded[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# D. Define Batch Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# D1. A Generator Function that returns an Iterator for Batches\n",
    "def get_batches(arr, n_seqs, n_steps):\n",
    "    '''\n",
    "    Desc : Generate batches of size n_seqs * n_steps\n",
    "    Variables :\n",
    "        - arr     : Input array\n",
    "        - n_seqs  : Sequences per batch (batch size)\n",
    "        - n_steps : Sequence steps per batch\n",
    "    '''\n",
    "    # Get the batch size and number of batches we can make\n",
    "    batch_size = n_seqs * n_steps \n",
    "    n_batches  = len(arr) // batch_size\n",
    "    \n",
    "    # Keep only enough characters to make full batches\n",
    "    arr =  arr[:n_batches * batch_size]\n",
    "    \n",
    "    # Reshape into n_seqs rows\n",
    "    arr = arr.reshape((n_seqs,-1))\n",
    "    \n",
    "    for n in range(0, arr.shape[1], n_steps):\n",
    "        # The features\n",
    "        x = arr[:,n:n+n_steps]\n",
    "        # The targets, shifted by one\n",
    "        y = np.zeros(x.shape)\n",
    "        y[:,:-1],y[:,-1] = x[:,1:] ,x[:,0]\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# D2. Validate the Function\n",
    "batches = get_batches(encoded, 10, 50)\n",
    "x, y = next(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# E. Define Model Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# E1. Shaping input placeholders and preparing for optimization\n",
    "def build_inputs(batch_size, num_steps):\n",
    "    '''\n",
    "    Desc : Tensorflow Placeholders\n",
    "    Variables :\n",
    "        - batch_size : Number of sequences per batch\n",
    "        - num_steps  : Sequence steps per batch\n",
    "    '''\n",
    "    # Graph placeholders\n",
    "    inputs = tf.placeholder(tf.int32,[batch_size,num_steps],name=\"inputs\")\n",
    "    targets = tf.placeholder(tf.int32,[batch_size,num_steps],name=\"targets\")\n",
    "    \n",
    "    # Retain probability placeholder for drop out layers\n",
    "    keep_prob = tf.placeholder(tf.float32,name=\"keep_prob\") # Scalar\n",
    "    \n",
    "    return inputs, targets, keep_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# F. Define LSTM Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# F1. Basic LSTM Cells\n",
    "def build_lstm(lstm_size, num_layers, batch_size, keep_prob):\n",
    "    '''\n",
    "    Desc : LSTM Cell for Hidden Layers\n",
    "    Variables :\n",
    "        - keep_prob  : Dropout optimization (scalar placeholder)\n",
    "        - lstm_size  : Size of the hidden layers in the LSTM cells\n",
    "        - num_layers : Number of LSTM layers\n",
    "        - batch_size : Batch size\n",
    "    '''\n",
    "\n",
    "    # LSTM cell and dropout to the cell outputs\n",
    "    # Stack LSTM layers, for vector ops (syntactic sugar)\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.DropoutWrapper\\\n",
    "                                       (tf.contrib.rnn.BasicLSTMCell(lstm_size)) \\\n",
    "                                        for _ in range(num_layers)])\n",
    "    # Fill the initial state with Zeros\n",
    "    initial_state = cell.zero_state(batch_size,tf.float32)\n",
    "    \n",
    "    return cell, initial_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# G. Define Model Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# G1. RNN Softmax Output Layer\n",
    "def build_output(lstm_output, in_size, out_size):\n",
    "    '''\n",
    "    Desc : Softmax layer that returns softmax output and logits (logarithm of the odds p/(1 âˆ’ p))\n",
    "    Variables: \n",
    "        - lstm_output : Output tensor list from LSTM layer\n",
    "        - in_size     : Size of the input tensor, for example, size of the LSTM cells\n",
    "        - out_size    : Size of this softmax layer\n",
    "    '''\n",
    "    # Reshape output: one row for each step for each sequence.\n",
    "    # Concatenate lstm_output over axis 1 (the columns)\n",
    "    seq_output = tf.concat(lstm_output,axis=1)\n",
    "    # Reshape seq_output to 2D tensor with lstm_size columns\n",
    "    x = tf.reshape(seq_output,[-1,in_size])\n",
    "    \n",
    "    # Connect RNN outputs to Softmax Layer\n",
    "    with tf.variable_scope('softmax'):\n",
    "        # Weight and Bias\n",
    "        softmax_w = tf.Variable(tf.truncated_normal((in_size, out_size),stddev=0.1))\n",
    "        softmax_b = tf.Variable(tf.zeros([out_size]))\n",
    "    \n",
    "    # Outputs are RNN cell output rows, therefore logits are output rows (one for each step and sequence)\n",
    "    logits =  tf.add(tf.matmul(x,softmax_w),softmax_b) \n",
    "    \n",
    "    # Softmax for predicted character probabilities\n",
    "    out = tf.nn.softmax(logits,name =\"out\")\n",
    "    print(out)\n",
    "    return out, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# H. Define Model Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# H1. Discover mean to calculate loss\n",
    "def build_loss(logits, targets, lstm_size, num_classes):\n",
    "    '''\n",
    "    Desc : Calculate loss from logits and targets\n",
    "    Variables : \n",
    "        - logits      : Logits from final fully connected layer\n",
    "        - targets     : Targets for supervised learning\n",
    "        - lstm_size   : Number of LSTM hidden units\n",
    "        - num_classes : Number of classes in targets\n",
    "    '''\n",
    "    # One-hot encode targets and reshape to match logits, one row per sequence per step\n",
    "    y_one_hot = tf.one_hot(targets,num_classes)\n",
    "    y_reshaped =  tf.reshape(y_one_hot,logits.get_shape())\n",
    "    \n",
    "    # Softmax cross entropy loss\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits,labels=y_reshaped))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I. Define Model Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I1. Clip Gradients to Ensure non-exponential growth\n",
    "def build_optimizer(loss, learning_rate, grad_clip):\n",
    "    '''\n",
    "    Desc : Build optmizer for training, using gradient clipping\n",
    "    Variables : \n",
    "        - loss: Network loss\n",
    "        - learning_rate: Learning rate for optimizer\n",
    "    '''\n",
    "    \n",
    "    # Optimizer for training (clipping the exploding gradients)\n",
    "    tvars = tf.trainable_variables()\n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars), grad_clip)\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "    optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "    \n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# J. The LSTM Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# J1. A class to define our model\n",
    "class CharRNN:\n",
    "    \n",
    "    def __init__(self,\n",
    "                 num_classes,\n",
    "                 batch_size=64,\n",
    "                 num_steps=50, \n",
    "                 lstm_size=128,\n",
    "                 num_layers=2,\n",
    "                 learning_rate=0.001, \n",
    "                 grad_clip=5,\n",
    "                 sampling=False):\n",
    "    \n",
    "        # Sampling: Pass one character at a time\n",
    "        if sampling == True:\n",
    "            batch_size, num_steps = 1, 1\n",
    "        else:\n",
    "            batch_size, num_steps = batch_size, num_steps\n",
    "\n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        # Build the input placeholder tensors\n",
    "        self.inputs, self.targets, self.keep_prob = build_inputs(batch_size,num_steps)\n",
    "        # Build the LSTM cell\n",
    "        cell, self.initial_state = build_lstm(lstm_size,num_layers,batch_size,self.keep_prob)\n",
    "        # (Run the data through the RNN layers)\n",
    "        # One-hot encode the input tokens\n",
    "        x_one_hot = tf.one_hot(self.inputs,num_classes)\n",
    "        \n",
    "        # Run each sequence step through the RNN \n",
    "        outputs, state = tf.nn.dynamic_rnn(cell,x_one_hot,initial_state=self.initial_state)\n",
    "        self.final_state = state\n",
    "        \n",
    "        # Softmax predictions and logits\n",
    "        self.prediction, self.logits = build_output(outputs,lstm_size,num_classes)\n",
    "        \n",
    "        # Loss and optimizer (with gradient clipping)\n",
    "        self.loss =  build_loss(self.logits,self.targets,lstm_size,num_classes)\n",
    "        self.optimizer = build_optimizer(self.loss,learning_rate,grad_clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# II. Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A. Define Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A1. Larger Networks + Dropout Values (0.0 - 1.0)\n",
    "batch_size = 100        # Sequences per batch\n",
    "num_steps  = 150        # Sequence steps per batch\n",
    "lstm_size  = 550        # LSTMs' Hidden layer sizes\n",
    "num_layers = 2          # LSTM layers\n",
    "learning_rate = 0.001   # Learning rate\n",
    "keep_prob  = 0.5        # Dropout 'keep_prob' probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# B. Model Training and Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# B1. Declare Epochs\n",
    "epochs = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# B2. Savepoints every n iterations\n",
    "save_every_n = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/dataframe/transforms/batch.py:41: parameter (from tensorflow.contrib.learn.python.learn.dataframe.transform) is deprecated and will be removed after 2017-06-15.\n",
      "Instructions for updating:\n",
      "contrib/learn/dataframe/** is deprecated.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/dataframe/transforms/batch.py:45: parameter (from tensorflow.contrib.learn.python.learn.dataframe.transform) is deprecated and will be removed after 2017-06-15.\n",
      "Instructions for updating:\n",
      "contrib/learn/dataframe/** is deprecated.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/dataframe/transforms/batch.py:49: parameter (from tensorflow.contrib.learn.python.learn.dataframe.transform) is deprecated and will be removed after 2017-06-15.\n",
      "Instructions for updating:\n",
      "contrib/learn/dataframe/** is deprecated.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/dataframe/transforms/batch.py:116: parameter (from tensorflow.contrib.learn.python.learn.dataframe.transform) is deprecated and will be removed after 2017-06-15.\n",
      "Instructions for updating:\n",
      "contrib/learn/dataframe/** is deprecated.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/dataframe/transforms/batch.py:120: parameter (from tensorflow.contrib.learn.python.learn.dataframe.transform) is deprecated and will be removed after 2017-06-15.\n",
      "Instructions for updating:\n",
      "contrib/learn/dataframe/** is deprecated.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/dataframe/transforms/csv_parser.py:53: parameter (from tensorflow.contrib.learn.python.learn.dataframe.transform) is deprecated and will be removed after 2017-06-15.\n",
      "Instructions for updating:\n",
      "contrib/learn/dataframe/** is deprecated.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/dataframe/transforms/csv_parser.py:57: parameter (from tensorflow.contrib.learn.python.learn.dataframe.transform) is deprecated and will be removed after 2017-06-15.\n",
      "Instructions for updating:\n",
      "contrib/learn/dataframe/** is deprecated.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/dataframe/transforms/example_parser.py:60: parameter (from tensorflow.contrib.learn.python.learn.dataframe.transform) is deprecated and will be removed after 2017-06-15.\n",
      "Instructions for updating:\n",
      "contrib/learn/dataframe/** is deprecated.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/dataframe/transforms/in_memory_source.py:52: parameter (from tensorflow.contrib.learn.python.learn.dataframe.transform) is deprecated and will be removed after 2017-06-15.\n",
      "Instructions for updating:\n",
      "contrib/learn/dataframe/** is deprecated.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/dataframe/transforms/in_memory_source.py:56: parameter (from tensorflow.contrib.learn.python.learn.dataframe.transform) is deprecated and will be removed after 2017-06-15.\n",
      "Instructions for updating:\n",
      "contrib/learn/dataframe/** is deprecated.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/dataframe/transforms/in_memory_source.py:60: parameter (from tensorflow.contrib.learn.python.learn.dataframe.transform) is deprecated and will be removed after 2017-06-15.\n",
      "Instructions for updating:\n",
      "contrib/learn/dataframe/** is deprecated.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/dataframe/transforms/in_memory_source.py:64: parameter (from tensorflow.contrib.learn.python.learn.dataframe.transform) is deprecated and will be removed after 2017-06-15.\n",
      "Instructions for updating:\n",
      "contrib/learn/dataframe/** is deprecated.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/dataframe/transforms/in_memory_source.py:68: parameter (from tensorflow.contrib.learn.python.learn.dataframe.transform) is deprecated and will be removed after 2017-06-15.\n",
      "Instructions for updating:\n",
      "contrib/learn/dataframe/** is deprecated.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/dataframe/transforms/in_memory_source.py:72: parameter (from tensorflow.contrib.learn.python.learn.dataframe.transform) is deprecated and will be removed after 2017-06-15.\n",
      "Instructions for updating:\n",
      "contrib/learn/dataframe/** is deprecated.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/dataframe/transforms/in_memory_source.py:76: parameter (from tensorflow.contrib.learn.python.learn.dataframe.transform) is deprecated and will be removed after 2017-06-15.\n",
      "Instructions for updating:\n",
      "contrib/learn/dataframe/** is deprecated.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/dataframe/transforms/in_memory_source.py:80: parameter (from tensorflow.contrib.learn.python.learn.dataframe.transform) is deprecated and will be removed after 2017-06-15.\n",
      "Instructions for updating:\n",
      "contrib/learn/dataframe/** is deprecated.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/dataframe/transforms/in_memory_source.py:84: parameter (from tensorflow.contrib.learn.python.learn.dataframe.transform) is deprecated and will be removed after 2017-06-15.\n",
      "Instructions for updating:\n",
      "contrib/learn/dataframe/** is deprecated.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/dataframe/transforms/reader_source.py:78: parameter (from tensorflow.contrib.learn.python.learn.dataframe.transform) is deprecated and will be removed after 2017-06-15.\n",
      "Instructions for updating:\n",
      "contrib/learn/dataframe/** is deprecated.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/dataframe/transforms/reader_source.py:82: parameter (from tensorflow.contrib.learn.python.learn.dataframe.transform) is deprecated and will be removed after 2017-06-15.\n",
      "Instructions for updating:\n",
      "contrib/learn/dataframe/** is deprecated.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/dataframe/transforms/reader_source.py:86: parameter (from tensorflow.contrib.learn.python.learn.dataframe.transform) is deprecated and will be removed after 2017-06-15.\n",
      "Instructions for updating:\n",
      "contrib/learn/dataframe/** is deprecated.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/dataframe/transforms/reader_source.py:90: parameter (from tensorflow.contrib.learn.python.learn.dataframe.transform) is deprecated and will be removed after 2017-06-15.\n",
      "Instructions for updating:\n",
      "contrib/learn/dataframe/** is deprecated.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/dataframe/transforms/reader_source.py:94: parameter (from tensorflow.contrib.learn.python.learn.dataframe.transform) is deprecated and will be removed after 2017-06-15.\n",
      "Instructions for updating:\n",
      "contrib/learn/dataframe/** is deprecated.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/dataframe/transforms/reader_source.py:98: parameter (from tensorflow.contrib.learn.python.learn.dataframe.transform) is deprecated and will be removed after 2017-06-15.\n",
      "Instructions for updating:\n",
      "contrib/learn/dataframe/** is deprecated.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/dataframe/transforms/reader_source.py:102: parameter (from tensorflow.contrib.learn.python.learn.dataframe.transform) is deprecated and will be removed after 2017-06-15.\n",
      "Instructions for updating:\n",
      "contrib/learn/dataframe/** is deprecated.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/dataframe/transforms/reader_source.py:106: parameter (from tensorflow.contrib.learn.python.learn.dataframe.transform) is deprecated and will be removed after 2017-06-15.\n",
      "Instructions for updating:\n",
      "contrib/learn/dataframe/** is deprecated.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/dataframe/transforms/reader_source.py:110: parameter (from tensorflow.contrib.learn.python.learn.dataframe.transform) is deprecated and will be removed after 2017-06-15.\n",
      "Instructions for updating:\n",
      "contrib/learn/dataframe/** is deprecated.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/dataframe/transforms/reader_source.py:114: parameter (from tensorflow.contrib.learn.python.learn.dataframe.transform) is deprecated and will be removed after 2017-06-15.\n",
      "Instructions for updating:\n",
      "contrib/learn/dataframe/** is deprecated.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/dataframe/transforms/sparsify.py:39: parameter (from tensorflow.contrib.learn.python.learn.dataframe.transform) is deprecated and will be removed after 2017-06-15.\n",
      "Instructions for updating:\n",
      "contrib/learn/dataframe/** is deprecated.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/dataframe/transforms/binary_transforms.py:78: parameter (from tensorflow.contrib.learn.python.learn.dataframe.transform) is deprecated and will be removed after 2017-06-15.\n",
      "Instructions for updating:\n",
      "contrib/learn/dataframe/** is deprecated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/requests/__init__.py:80: RequestsDependencyWarning: urllib3 (1.22) or chardet (2.3.0) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"out:0\", shape=(15000, 76), dtype=float32)\n",
      "WARNING:tensorflow:Error encountered when serializing LAYER_NAME_UIDS.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'dict' object has no attribute 'name'\n"
     ]
    }
   ],
   "source": [
    "# B3. Try first with one GPU \n",
    "with tf.device(\"/gpu:0\"):\n",
    "    model = CharRNN(len(vocab),\n",
    "                    batch_size=batch_size,\n",
    "                    num_steps=num_steps,\n",
    "                    lstm_size=lstm_size,\n",
    "                    num_layers=num_layers, \n",
    "                    learning_rate=learning_rate)\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=100)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Load a checkpoint and resume training: saver.restore(sess, 'checkpoints/______.ckpt')\n",
    "    counter = 0\n",
    "    for e in range(epochs):\n",
    "        # Train network\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        loss = 0\n",
    "        for x, y in get_batches(encoded, batch_size, num_steps):\n",
    "            counter += 1\n",
    "            start = time.time()\n",
    "            feed = {model.inputs: x,\n",
    "                    model.targets: y,\n",
    "                    model.keep_prob: keep_prob,\n",
    "                    model.initial_state: new_state}\n",
    "            batch_loss, new_state, _ = sess.run([model.loss, \n",
    "                                                 model.final_state, \n",
    "                                                 model.optimizer], \n",
    "                                                 feed_dict=feed)\n",
    "            \n",
    "            end = time.time()\n",
    "            print('Epoch: {}/{}... '.format(e+1, epochs),\n",
    "                  'Training Step: {}... '.format(counter),\n",
    "                  'Training loss: {:.4f}... '.format(batch_loss),\n",
    "                  '{:.4f} sec/batch'.format((end-start)))\n",
    "        \n",
    "            if (counter % save_every_n == 0):\n",
    "                saver.save(sess, \"checkpoints/i{}_l{}.ckpt\".format(counter, lstm_size))\n",
    "    \n",
    "    saver.save(sess, \"checkpoints/i{}_l{}.ckpt\".format(counter, lstm_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# C. Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# C1. Next Char\n",
    "def pick_top_n(preds, vocab_size, top_n=5):\n",
    "    '''\n",
    "    Desc : Feed in char, receive next char.\n",
    "    Variables:\n",
    "        - preds      : Char to predict\n",
    "        - vocab_size : Length of the vocab\n",
    "        - top_n      : Return the top selection 'n' of chars\n",
    "    '''\n",
    "    p = np.squeeze(preds)\n",
    "    p[np.argsort(p)[:-top_n]] = 0\n",
    "    p = p / np.sum(p)\n",
    "    c = np.random.choice(vocab_size, 1, p=p)[0]\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# C2. Sample based on Checkpoint\n",
    "def sample(checkpoint, n_samples, lstm_size, vocab_size, prime=\"The \"):\n",
    "    '''\n",
    "    Desc : Resturn a sample from a checkpoint restore\n",
    "    Variables:\n",
    "        - checkpoint : ckpt file to restore\n",
    "        - n_samples  : Number of samples to return\n",
    "        - lstm_size  : LSTM size\n",
    "        - vocab_size : Vocabulary size\n",
    "        - prime      : Prime the results with a string\n",
    "    '''\n",
    "    samples = [c for c in prime]\n",
    "    model = CharRNN(len(vocab), lstm_size=lstm_size, sampling=True)\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, checkpoint)\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        for c in prime:\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0,0] = vocab_to_int[c]\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.prediction, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "        c = pick_top_n(preds, len(vocab))\n",
    "        samples.append(int_to_vocab[c])\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            x[0,0] = c\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.prediction, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "            c = pick_top_n(preds, len(vocab))\n",
    "            samples.append(int_to_vocab[c])\n",
    "        \n",
    "    return ''.join(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"out:0\", shape=(1, 76), dtype=float32)\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/i0_l550.ckpt\n",
      "Far'6yTrC<<PTT-jyyhU)Mh1OUOy1yFllUUUlUjj4UUl1U1UU(Ul11%%11%jjvv))psvUpsb1RhbP1obb'aPTvvaCaaTcjc-bcBb1DbvoDorv0vobo%b%8a28vamBDcUcBvbpDobpUUooUjVbjaV(aaajUUh1UU1OUjbj1jcl1l1c1U1v1ctb1c1b1bbbv'vov0o8bbm88BDBDTvvXDvvo4444\t\ta\t8\taAbDhAhTPTA-:ryywVVyLaaUaU5<hO-hhjj:Vl)lUV1Vl1bbUU1XHHXXKjKl1lGX1lmGXLGmmmWmnmnmYUWYFYfFcfftc/BVV\n",
      "Lc\n",
      "\n",
      "D\n",
      "CSSCW%Okx%bFy.7NZa.+obb(oVfajaVjTT1VBbUbUcb1BvvvccvpbDD1oo818TTvPaPBOaFc-cbb:bDbD(oor8D%88B-&Dbv&bXCXC<FFvDByrr.byTyTVyrCbPCkCy<OCy.j-Zg&rr\t\tbD0((o(o+PDaVVVajc<cc-h1-cch&&\n",
      "hb\n",
      "S\n",
      "D\n",
      "SM\n",
      "SWC%4%4%55CaCCk.-k.c<e2-ZV9\n",
      "wRRRlllfIlfwwII1(1wwwLLxxQUh+)Uh00VmOFVVa\n",
      "Xa\n",
      "-c\n",
      "-bX\n",
      "\n",
      "\n",
      "b 6 MX%%%jj -2-aa9Vyywh<hhrrhVVyyyVOy\n",
      "\n",
      "R\n",
      "\n",
      "yRyn<<Cj<UUj(jVK))((KU9BBH77(GvvvGvmcmmbvTTT1vb9b\ttt\ta0aaccDcc11ccc1cbtLLbaoaaabbojjAT:+rUTrrr8VV<<jq<jUjUyPy1pp1U1U1bPPUjvjvv1pp)1OOSTt1HbFvFHTaTv+:bamccVbDcob1bVDoboaaaojcUjUVAAAVaac-f-+bch\n",
      "XhD&DX&X&nFlFY%Nl%nlBlldUtwlIwLUlllUwUhw1wlUlL%UUjpl1ppVUn11FbUFHH'lbfma\taVGVaGcKBcciLLibm\n",
      "L\n",
      "CmCgCSgm.g%%a%99u9nsn\tYwYwsWW/tVAV5QOKp5R-RRIpww(wIoh%sah%-hhh--yyrOV0XOX44--Z\n",
      "hwhMwXWyW)h1))VOKtL9oa0LHH00+FmxammcFx+TTxfffc\n",
      "VbV\n",
      "B\n",
      "\n",
      "byyy\n",
      "\n",
      "y\n",
      "My6My6MUYYyNOYMMjNaOj vvUymyKmmUmMUv9vmvvmbvv9)9m\t\tbaD(9abbD(PTTjaTTb1:BBPbTPcT1P.aajj1V1TcTbcbvvB1bvoottDa0aa--h--UcrXbhUbjVXXlVXjX1XKX\n",
      "1\n",
      "1\n",
      "\n",
      "XbXvvvmvvoc1b8m8vBTTBbvbtootD90o0H0V0mOmxVxmmFxxRx+fVV\n",
      "Vf\n",
      "OxbVxVx\n",
      "kxbxbyVVBVb\n",
      "XX<RX<yC.<k<<V-Gj(K(Rii)lol7L7lmUwUU(MlalUnnnUlllMwMj1lll11lPPwmLUP11mmm%baUba()\tU\taffcajc1TcccEcBbL\n",
      "1LSbbotbt D%oCC%%a%abjaa<<<-<i-iiUj<<U)Uhhhh)UjlM)h11OlOUHHHHFaFllllmUmynyUyyMUympMmMUM9YnYf11c1OHcvt1cvv1Smtcbtm0m0m\tacFac9ccb1ic\n",
      "biibooC.bbCC8778C7eKXX7K7.GKwX7wvXvwvyGGmKhKmmLaTLSamTfmmfWWTM1\tT51\t9\n",
      "t10PmP999wwwxQvvw)vpvpoTUh%bh111U%FaHbTXHHaXXTNN-B\n",
      "Xc\n",
      "XXX\n",
      "XynXC.CkFNeckcV7V.\n",
      "BNRooHNHNNR-ffhAfVOQVOCOCCxRFkxbbxV..(+BZFFx...NNfDNNBTooBfoTbfToVfVBOOxTHRxbaxyFxFFy+T6+fThZVVabackhVDBhhD\n",
      "DyOlTolPlllPl4UmmnmyyUmUmUnUf(fff\tM)(H)(H)VFqHLqLLlaSSaLcrLLmmr+rUUb+UmU(f+)7f)H)(LoHHVqfaOLVmKmxLacUUcbUb1bU\tbjaabVVjj<j<ccicpVi\n",
      "BbpoooS2SbWWCf%%77Ka7((Ci7aCCX<<Fj<-UUjhhU)hUjjV1llOV1911wUUHHbv(aFbFo66UVaVUK((Kaljaajj-chGhUhUlrVU1llUlUU\n"
     ]
    }
   ],
   "source": [
    "checkpoint = tf.train.latest_checkpoint('checkpoints')\n",
    "samp = sample(checkpoint, 2000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
